# Evolving the Prototype Journey:  Lifelong Vision-and-Language Navigation with Prototype Adaptation



<div align="center">
<img src="assets/Figure.png" width="90%">
</div>


---

## ğŸ“Œ Overview

**EverWalker** is a novel framework for **lifelong vision-and-language navigation (LVLN)** that enables navigation agents to continually learn new tasks without catastrophic forgetting. Our method achieves state-of-the-art performance through three key innovations:

<div align="center">
<img src="assets/overview.png" width="90%">
<p><i>ProtoStream enables lifelong learning across diverse navigation scenes and instruction styles</i></p>
</div>

- ğŸ§© **Dynamic Prototype Bank**: Automatically grows to capture scene knowledge with soft routing mechanism
- ğŸ”§ **HyperNetwork**: Generates step-level LoRA adaptations conditioned on weighted prototypes
- ğŸ¯ **Multi-Level Distillation**: Novel HyperNet output distillation to prevent both prototype drift and mapping instability


---

## âœ¨ Key Features

### Lifelong Learning without Forgetting
- âœ… **Only 4.3% forgetting rate** (11% improvement over baselines)
- âœ… **67.3% average success rate** across 18 continual tasks
- âœ… **Strong zero-shot generalization** to unseen scenes

### Efficient and Scalable
- âš¡ **4% computational overhead**: Minimal additional cost
- ğŸ”„ **Dynamic adaptation**: Step-level LoRA generation

### Comprehensive Framework
- ğŸ“ Based on StreamVLN with Qwen-7B backbone
- ğŸ—ï¸ Modular design: Easy to extend and customize
- ğŸ“Š Complete evaluation suite with multiple metrics

---

## ğŸ—ï¸ Architecture

### System ComponentsKey Innovations

**1. Dynamic Prototype Bank**

```python
# Soft routing over ALL prototypes (not top-k)
similarities = cosine_similarity(z_t, prototypes)  # (K,)
weights = softmax(similarities / temperature)       # (K,)
weighted_proto = sum(weights * prototypes)         # (512,)
```

**2. HyperNetwork Design**

```python
# Step-level LoRA generation
for layer_size in unique_sizes:
    lora_A = generator_A(weighted_proto)  # rank Ã— in_dim
    lora_B = generator_B(weighted_proto)  # out_dim Ã— rank
```

**3. Multi-Level Distillation**
```python
# Complete distillation chain
L_total = L_task                    # Task loss
        + Î»_sp * L_sp              # Single-proto KL
        + Î»_pp * L_pp              # Proto-pair cosine
        + Î»_cp * L_cp              # Cross-proto MSE
        + Î»_lora * L_lora          # HyperNet output (NEW!)
        + Î»_div * L_div            # Diversity loss
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.7+ (for GPU support)
- 8Ã— NVIDIA A6000 GPUs (or equivalent, 48GB VRAM each)

### Installation

```bash
# Clone the repository
git clone https://github.com/Lifelong-EverWalker/EverWalker.git
cd EverWalker

# Create conda environment
conda create -n protostream python=3.8
conda activate EverWalker

# Install dependencies
pip install -r requirements.txt

# Install habitat-sim (for VLN simulation)
conda install habitat-sim -c conda-forge -c aihabitat
```

### Quick Start

#### 1. Prepare Data

```bash
# Download StreamVLN dataset
python scripts/download_dataset.py

# Preprocess data
python scripts/preprocess_data.py
```

#### 2. Train EverWalker

```bash
# Single GPU training (for debugging)
python streamvln_train.py \
    --config config/vln_r2r.yaml \
    --use_protostream \
    --output_dir outputs/protostream_debug

# Multi-GPU training (recommended)
bash scripts/streamvln_train_protostream.sh
```

#### 3. Evaluate

```bash
# Evaluate on all tasks
python streamvln_eval.py \
    --checkpoint outputs/protostream/checkpoints/task_1.pth \
    --config config/vln_r2r.yaml \
    --output_dir outputs/evaluation
```


---

## ğŸ“ Project Structure

```
EverWalker/
â”œâ”€â”€ config/                          # Configuration files
â”‚   â””â”€â”€ vln_r2r.yaml                # StreamVLN R2R config
â”‚
â”œâ”€â”€ llava/                           # Vision-language model
â”‚
â”œâ”€â”€ scripts/                         # Training and evaluation scripts
â”‚   â”œâ”€â”€ streamvln_train_protostream.sh
â”‚   â”œâ”€â”€ zero2.json                  # DeepSpeed config
â”‚   â””â”€â”€ zero3.json
â”‚
â”œâ”€â”€ streamvln/                       # Main source code
â”‚   â”œâ”€â”€ model/                       # Model implementations
â”‚   â”‚   â”œâ”€â”€ continual_learning.py   # Continual learning utils
â”‚   â”‚   â”œâ”€â”€ hyper_lora_layers.py    # HyperNetwork generators
â”‚   â”‚   â”œâ”€â”€ prototype_manager.py    # Prototype bank management
â”‚   â”‚   â”œâ”€â”€ scene_encoder.py        # Scene feature encoder
â”‚   â”‚   â”œâ”€â”€ triple_distillation.py  # Multi-level distillation
â”‚   â”‚   â””â”€â”€ stream_video_vln.py     # Main VLN model
â”‚   â”‚
â”‚   â”œâ”€â”€ dataset/                     # Data processing
â”‚   â”œâ”€â”€ habitat_extensions/          # Habitat simulator extensions
â”‚   â””â”€â”€ utils/                       # Utility functions
â”‚       â”œâ”€â”€ dist.py                  # Distributed training
â”‚       â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ streamvln_train.py              # Training script
â”œâ”€â”€ streamvln_eval.py               # Evaluation script
â”œâ”€â”€ streamvln_eval_baseline.py      # Baseline evaluation
â”œâ”€â”€ args.py                          # Argument parser
â”œâ”€â”€ measures.py                      # Evaluation metrics
â”œâ”€â”€ maps.py                          # Navigation maps
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

---

## ğŸ¯ Reproducing Results

### Expected Outputs

After training completes, you should see:

```
outputs/
â”œâ”€â”€ EverWalker/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ task_1.pth
â”‚   â”‚   â”œâ”€â”€ task_2.pth
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ prototypes/
â”‚   â”‚   â”œâ”€â”€ task_1_protos.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ teacher_loras/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ training.log
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ task_wise_sr.json
    â”œâ”€â”€ forgetting_rate.json
    â””â”€â”€ ablation_results.json
```
